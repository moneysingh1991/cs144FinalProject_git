HETER0 ASSOCIATIVE NEURAL NETWORK 
FOR PATIERN RECOGNITION 
Taiwei Lu, Xin Xu, Shudong Wu, and Francis T. S .  Yu 
Department of Electrical Engineering 
The Pennsylvania State University 
University Park, PA 16802 
ABSTRACT 
We present an Inter-Pattern Association (IPA) 
n e u r a l  n e t w o r k  m o d e l ,  in  w h i c h  b a s i c  l o g i c a l  
operations are used to determine the association 
among common and spec ia l  f ea tu re s  o f  reference 
pat terns  (i .e. ,  in ter-pat tern associat ion) .  Hetero-  
and  au to -as soc ia t ive  memory  a r e  syn thes i zed  by 
a p p l y i n g  a g e n e r a l i z e d  l o g i c a l  ru l e .  Compute r  
simulations for pattern recognition by using the IPA 
mode l  h a v e  s h o w n  a b e t t e r  p e r f o r m a n c e  and  a 
higher storage capacity than Hopfield model. A 2-D 
adaptive optical  neural network is  used to perform 
p a r a l l e l  n e u r o c o m p u t a t i o n s .  S i n c e  t h e  
Interconnection Weight Matrix (IWM) for IPA model 
has a tr i-state structure,  the dynamic range imposed 
on a Spatial Light Modulator (SLM) is rather relaxed, 
and the interconnect ions are  much s impler  than the  
Hopfield model. 
In  this  paper,  we shall  present a neural  network 
model based on the association between reference 
patterns. First ,  th is  model determines the common 
and spec ia l  f ea tu re s  among pa t t e rns  by app ly ing  
logical operations on the pixels in the pattern space. 
Based on the relationship between the special  and 
common features ,  two equivalent logical rules  are 
presented in Sec. I1 to  construct the excitatory and 
inhibitory interconnections in the network. In Sec. 
111. the Inter-Pattern Association (IPA) model is 
c o m p a r e d  w i t h  t h e  H o p f i e l d  m o d e l  i n  t h e  
performance ve r sus  number of reference pa t t e rns  
in a noisy environment.  Computer s imulat ions of 
IPA model are conducted for  recognizing tools  of 
different orientations.  In Sec.  IV, a 2-D hybrid 
p a r a l l e l  o p t i c a l  n e u r a l  ne twork  i s  u sed  f o r  t h e  
implementation of IPA and Hopfield models.  The 
experimental  results show that IPA model performs 
more effectively than Hopfield model. 
I. INTRODUCTION 11. IPA MODEL, AND ASSOCIATIVE MEMORY 
The superior  capabili ty of human perception in 
p a t t e r n  r e c o g n i t i o n  h a s  s t i m u l a t e d  a g r e a t  
e n t h u s i a s m  a m o n g  n e u r a l  n e t w o r k  r e s e a r c h e r s .  
Mathematical  modeling of various neural  networks 
has been developed during the past  several decades 
[l-41. However, for recognition of a given set  of 
r e f e r e n c e  p a t t e r n s ,  m a n y  n e u r a l  n e t w o r k  
a lgo r i thms  cons t ruc t  t he  In t e rconnec t ion  Weight  
Matrix (IWM) by emphasizing on the association of 
the elements within each reference pattern (i .e. ,  the 
i n t r a - p a t t e r n  a s s o c i a t i o n ) ,  w h i l e  p a y i n g  l i t t l e  
attention to  the association between patterns (i .e. ,  
inter-pattern association). For example, in Hopfield 
type neural networks[5],  the outer-products of the 
reference patterns are added to form the IWM. This 
type of approach is  based on the assumption that  
r e f e r e n c e  p a t t e r n s  a r e  s i g n i f i c a n t l y  d i f f e r e n t .  
However ,  in pract ice ,  the reference pat terns  are  
usually not independent,  and the differences among 
the patterns are often very small  (e.g. human faces,  
f inger pr ints ,  handwritten characters,  etc.). Thus 
it may create an unstable o r  ill-conditioned network. 
Under these circumstances.  the special  features of 
t h e  p a t t e r n s  p l a y  a n  i m p o r t a n t  r o l e  i n  p a t t e r n  
recognition. Therefore it i s  necessary to  consider 
t h e  r e l a t i o n s h i p s  b e t w e e n  t h e  common  and  t h e  
spec ia l  f ea tu re s  among the  r e fe rence  pa t t e rns  in  
constructing the IWM. 
T h e  in fo rma t ion  ob ta ined  from p ixe l s  in  the  
spec ia l  a r eas  of  a r e fe rence  pat tern i s  gene ra l ly  
more important  than that from the common areas  
( i . e .  a r e a s  sha red  by more  than  o n e  p a t t e r n )  i n  
terms of constructing the associative memory matrix 
for a neural  network. The basic concept of IPA 
model  i s  to  de t e rmine  whe the r  t he  p i x e l s  in  t h e  
pattern space belong to  special o r  common spaces of 
the reference patterns,  and then set  the excitatory 
o r  i n h i b i t o r y  i n t e r c o n n e c t i o n s  a c c o r d i n g  t o  a 
logical relationship. 
Figure 1 shows an example of a training set  that 
consists of three overlapping patterns A, B and C in 
space S 1  ( see  F i g . l ( a ) ) ,  and the i r  co r re spond ing  
desired output patterns A', B' and C' in space S2 (see 
F i g . l ( b ) ) .  T h e s e  p a t t e r n s  c a n  b e  d i v i d e  i n t o  7 
subspaces; For instance, in space SI .  subspaces I, I1 
and I11 are the special areas of patterns A, B and C. 
respectively. IV, V and VI are the common areas of 
A and B. B and C, C and A, respectively, while VI1 is 
the common area of A, B and C. And the rest can be 
def ined as  an empty space a. Similar ly ,  for  the 
subspaces in S2. as  illustrated in Fig. l (b) .  The 
s u b s p a c e s  of  s i  and S2 can be expressed by the 
fo 1 Io w in g logical functions : 
658 
number of reference patterns. Similarly,  the logical  
operation in subspace X '  in S2  can also be written in 
the same form-, such as: 
X = P A Q ,  (2') 
where P = p'1 A p'2 A . . . A pIn , (3') 
Q = q l l  v 4 '2  v . . . v q'm . (4') 
The  p ixe ls  in  a r e a  X must  excite ( i .e. having 
positive connections with) all the pixels in area P', 
inhibi t  (i.e. having  n 9 a t i v e  connections with) a l l  
the pixels in area Q' A P " ,  
and have n o  connection with the remaining areas in  
S2.  For simplicity, the connection strengths ( i.e. 
weights ) are  defined a s  '1' fo r  positive connections; 
'-1' f o r  n e g a t i v e  c o n n e c t i o n s ;  a n d  '0 '  f o r  n o  
connection. Thus  the IPA neural network can  b e  
constructed in a simple tri-state structure. 
where P" is  defined by 
P" = p'1 v p'2 v . . . v PIn, ( 5 )  
If patterns in  S 2  are  the same as  the patterns in  
S I .  then Rule A can be used to construct the auto- 
associative memory. 
To i l lustrate  fur ther  the construction of  the IPA 
model ,  w e  s h a l l  s y n t h e s i z e  an  a u t o - a s s o c i a t i v e  
memory for A, B and C,  three 2x2 array patterns as 
shown in Figs. 2(a), (b) and (c), respectively. The  
pixel-pattern relationship is  given in Table I. I t  i s  
apparent that  pixel  1 represents the common feature 
of A, B and C, pixel 2 is  the common feature of A and 
B, pixel 3 the common feature of A and C, and pixel 4 
represents the special feature of C. 
By  a p p l y i n g  R u l e  A t o  t h e  t h r e e  r e f e r e n c e  
patterns A, B and C,  a tri-state neural network can 
be constructed, as  illustrated in Fig. 2(d). This is  a 
s ing le  layer  neural  network with 4 neurons at  the 
input end  and 4 neurons  at the output  end. Each  
neuron i s  matched t o  one  pixel of the input/output 
p a t t e r n s .  F o r  e x a m p l e ,  t h e  1 s t  i n p u t  n e u r o n  
co r re sponds  t o  p i x e l  1 o f  the  i n p u t  pa t te rn ,  and  
excites only the 1st output neuron. The  2nd input 
n e u r o n  ( t h e  c o r r e s p o n d i n g  p i x e l  b e l o n g s  t o  
patterns A anb  B) exci tes  both the 1st and the 2nd 
o u t p u t  n e u r o n s ,  w h i l e  i n h i b i t i n g  t h e  4 t h  o u t p u t  
neuron, which belongs t o  the special area of pattern 
C. 
I t  i s  interesting to  analyze the s t ructure  of the 
IWM. Since the reference patterns are in 2-D form, 
the weight matrix becomes a 4-D matrix. We can 
partition the 4-D IWM into a 2-D submatrix array [ 6 ] ,  
as  illustrated in  Fig.2(e). The  IWM can be divided 
into 4 blocks. Each block corresponds to  one output 
neuron. The  4 elements in  one  block represent the 4 
neurons in the input end. For  example, s ince all the  
4 elements in the upper-left block have value 1 ,  it 
indicates that  e i ther  one  of  the 4 input neurons can 
exc i te  t h e  1st  ouput  neuron .  In the upper-right 
block, as  another example, the 1st and 3rd elememts 
a re  0, the 2nd element has value 1,  and the 4th 
element is  -1. From this example we can determine 
t h a t  t h e  1 s t  a n d  3 r d  i n p u t  n e u r o n s  h a v e  n o  
connection to the 2nd output neuron, the 2nd input 
neuron excites the 2nd output neuron, and the 4 th  
input neuron inhibi ts  the 2nd output neuron. 
In  order to simplify the algebraic operations, an 
equiva len t  rule ,  Rule  B, i s  developed as  fo l lows ,  
w h i c h  c a n  b e  u s e d  t o  c o n s t r u c t  t h e  I W M  by  
examining the pixel-pattern relationships. 
@ @  A' VII'  v VI  v 
V 
111 111' 
s1 s 2  
( a )  ( b )  
Fig.1: ( a )  C o m m o n  a n d  s p e c i a l  a r e a s  o f  t h r e e  
r e f e r e n c e  p a t t e r n s ,  ( b )  c o r r e s p o n d i n g  d e s i r e d  
output patterns. 
- 
I = A A ( B ~ ) ,  I ' = A A ( B ' V  e), -
I1 = B A (A v C). 11' = B' A ( A  v C'), 
I11 = C A (A v B), 111' = c' A (A' v B'), 
-
IV = (A AB)  A C  W = ( A  AB') AF, 
v = (B A C) A iC, v' = (B' A c) A iC, 
VI=(CAA)A%, W = ( C A A ' )  AFT, 
(1) 
vn = (A A B A C) A 5. VII' = ( A  A B' A c) AA 
where ' A I .  ' v ' and I - '  stand for the logic "AND" , 
"OR" and "NOT" operations, respectively. 
When a pixel in area VI1 i s  on (i.e. the  pixel has a 
value '1'). i t  implies that there  is  an input, but we 
can not judge whether i t  belongs to  any of the A, B, 
and C patterns. Thus  this pixel can only excite the 
p i x e l s  w i t h i n  a r e a  V I I ' ,  f o r  w h i c h  i t  h a s  n o  
connection with the pixels  in  other areas belong to  
space S2. 
i t  implies that the 
input pattern i s  not A, but we can not tell whether i t  
belongs to  pattern B or C. Thus this pixel should 
exci te  the pixels in areas V' and VII', but inhibit the 
pixels in  area 1'. Similarly, logical operations can 
also be applied to pixels in area IV or VI. 
Fo r  t h e  c a s e  when a pixel i s  on  in  area I, i t  
implies that pattern A is  appeared at the input end. 
Therefore ,  th i s  pixel  can  exc i te  a l l  the  pixels in 
pattern A' (i.e. areas 1'. IV', VI' and VII' ), but inhibit 
the pixels in areas ( B' v C' ) A T  (i.e. areas 11', 111' 
and V' ). Similarly, for the pixels in area I1 or I11 , 
they must excite area B' or C', and inhibit areas 1'. 
111' and VI', or areas 1'. 11' and IV', respectively. 
the descriptive logic can  b e  
summarized in  the following rule, which is  extented 
f o r  any number of  reference patterns: 
Rule A: 
An arbi t rary subspace X in  input space S1 c a n  
always be represented by the following expression: 
When a pixel in area V is  on, 
In  view of eq.( l ) .  
X = P A Q ,  (2)  
where P = p1 A p 2 A  . . .  A p n  , (3)  
Q = q l  v 4 2  v . . .  V Sm . (4 )  
p1. p2, . . . , pn. a re  input reference patterns that  
occupy the subspace X , ql .  q2 . .  . . qm are  input  
reference patterns that d o  not belong t o  X, n and m 
are  two  pos i t ive  integers.  and n+m = M the total  
659 
- 7-- 
A 
B 
C 
Input layer output layer 
(4 
1 2 3 4  
1 1 1 0  
1 1 0 0  
1 0 1 1  
Fig .2 :  C o n s t r u c t i n g  an au to -as soc ia t ive  memory  
matrix using IPA model: (a),  (b),  and (c) represent 
three 2x2 reference pat terns ,  (d)  a tr i-state neural  
network, (e) interconnection weight matrix.  
T a b l e  I: 
reference patterns in Fig.2.  
P ixe l -pa t t e rn  r e l a t ionsh ip  of  t he  th ree  
R U k E  
L e t  u s  d e f i n e  D l . i  a s  a 2 - D  ma t r ix  t h a t  
corresponds to the 2-D 'pixel-pattern array in table I, 
where 1 and i denote  the row and column number,  
and D i , i  as the  corresponding pixel-pat tern matrix 
for desired output patterns. Let di be the number o f  
patterns that are bright (i.e. in state '1' ) at the ith 
pixel ,  then i t  can be determined by summing the 
elements in the ith column of table I, i.e., 
di = c D,,i I 
I =1 
d: = g,, 
I L1 
Let us also define 
Then we shall  now construct an IPA neural network 
by applying the following logical rules: 
(1) If Ki, = di, this  means that the patterns 
sha r ing  the  ith pixel in S I  are included by patterns 
of which the corresponding patterns in S2 share  the 
j t h  pixel. thus pixel i in S1 should excite pixel j in  
(2) If 0 < K.. < di, this implies that some patterns 
sha r ing  the i t h  pixel do not share t h e j r h  pixel, thus 
the i t h  pixel in S1 can not excite the j t h  pixel in S2. 
s2; 
'J 
(3) If Kij =O,  
when d i  # 0, and d'.  z 0, which means that  no J 
common pattern shares pixels i and j ,  
in S1 must inhibit pixel j in S2; 
when d i  = 0 and/or d'.  = 0, then pixel i must have 
no connection to pixel j .  
We stress that ,  these logical rules for constructing 
the  IWMs are  r a the r  s t r a igh t  fo rward ,  which are  
suitable for computer implementations. Notice that, 
the IWMs computed by Rule B are equivalent  to  
those obtained by Rule A. 
then pixel i 
J 
111. COMPARISON WITH THE HOPFIELD MODEL 
It is the differences,  rather than the s imilar i t ies  
among pat terns ,  used fo r  pat tern recogni t ion.  For  
example, the outline of the eyes,  nose and mouth are 
c o m m o n  f e a t u r e s  i n  a l l  h u m a n  f a c e s .  P e o p l e  
dis t inguish d i f f e ren t  persons by the differences,  
rather than the detail of faces. 
S i m i l a r  t o  m a n y  o t h e r  n e u r a l  n e t w o r k  
algorithms, Hopfield model constructs the IWM by 
c o r r e l a t i n g  t h e  e l e m e n t s  w i t h i n  e a c h  p a t t e r n ,  
h o w e v e r ,  i g n o r i n g  t h e  r e l a t i o n s h i p s  among  t h e  
reference patterns. The IWM of Hopfield model for 
three reference patterns A, B and C can generally be 
expressed as 
where ' ' stands for  the transpose of the vectors 
and I i s  the unit  matr ix ,  which makes the  weight 
matrix zero-diagonal. 
If input pattern A is applied to the neural system, 
then the output would be 
T = AAT + BBT + c c T  - 31 ( 9 )  
V=TA 
= A (AT A) + B (BT A) + C (CT A) - 3A (10) 
w h e r e  AT A r e p r e s e n t s  t h e  a u t o c o r r e l a t i o n  of 
pattern A, while BT A and CT A are the respective 
cross-correlation between A and B, A and C. If the 
d i f f e rences  between A ,  B and C a re  su f f i c i en t ly  
large, the autocorrelation of A, B or C would be much 
larger than the cross-correlation between them, i.e., 
Notice that ,  from eq. (10). pattern A has a larger 
weighting factor than patterns B and C. for which 
patterns B and C can thus be considered as noise. By 
choosing the proper threshold value,  pattern A can 
be reconstructed at the ou tpu t  end  of t he  neural  
network. 
On the  o the r  hand,  if A, B and C are  c losely 
similar,  the inequalit ies of Eq. (11) are no longer 
he ld ,  s ince  the  we igh t s  of pa t t e rns  B and C are  
comparable with those of pattern A. Patterns B and C 
c a n  n o  l o n g e r  be  t r e a t e d  a s  n o i s e .  T h u s  t h e  
AT AX-BT A, AT AX-CT A .  ( 1  1 )  
660 
7 1  
threshold value for Hopfield model can not be easily 
d e f i n e d ,  and  t h e  neu ra l  ne twork  wou ld  become  
unstable.  
Computer simulations of a 2-D neural network, 
with an 8 x 8 array neurons at the input and output 
ends have been conducted for both Hopfield and IPA 
models. The reference patterns considered are the 26 
capital English letters,  lined up in sequence based on 
the similarities of the letters, ' B I ,  ' P ', ' R ' , ' F I, . . 
. , and each l e t t e r  occupies  with an 8 x 8 array 
pixels. 
F i g u r e  3 s h o w s  t h e  o u t p u t  e r r o r  r a t e s  a s  a 
function of reference patterns for Hopfield and IPA 
mode l s  unde r  noisy cond i t ions .  The  input  no i se  
levels are set at about 0%. 5 %  and 10%. respectively. 
We not ice  that ,  Hopfield model becomes unstable  
when four patterns ' B '. ' P ', ' R ' and 'F' are stored 
in the IWM, wheras IPA model is quite stable with 12 
s to red  l e t t e r s .  Under  the cond i t ion  of no i se l e s s  
input,  the IPA model can produce correct results for  
all 26  stored letters in the IWM, wheras Hopfield 
mode l  s t a r t s  mak ing  s ign i f i can t  e r ro r s  when t h e  
number  of  r e fe rence  p a t t e r n s  i s  i nc reased  t o  6 .  
From Fig. 3 we see that the IPA model is  more robust 
and it has a larger storage capacity as  compared with 
the Hopfield model. 
IPA model can be used to train neural networks to 
p e r f o r m  s h i f t  a n d  r o t a t i o n  i n v a r i a n t  p a t t e r n  
recognition, s ince the IPA neural network has large 
storge capacity and high robustness. 
HoDf ield 
4 
0 
0 10  20 
Reference Pattern Number 
30 
Fig.3: Comparison of IPA and Hopfield models; input 
noise levels: I: 0%. 11: 5%,  and 111: 10%. 
The neural network is trained to  recognize tools 
with different orientation, as shown in Fig.4. IPA 
model  i s  used to construct  an Hetero-associat ive 
memory of these tools.  The f i rs t  row in Fig.4(a) 
consists of 10 training patterns. The desired output 
must be the correct tool,  and always aligned in the 
up right direct ion,  as  shown in the second row of 
Fig.4(a). The interconnection weight matrix,  buil t  
by Rule B, are rather simple, as  shown in Fig.4(b), 
the dark parts are inhibitory weights (i .e. ,  -1). the 
bright parts are exitatory weights (i.e., + I ) ,  and the 
medium gray level represents no connection (i.e., 0). 
F i g u r e  4 ( c )  s h o w s  s o m e  s i m u l a t e d  r e s u l t s  
performed by the IPA neural network. The patterns 
in the  f i rs t  and third columns are  input pat terns ,  
e i the r  i ncomple t e  o r  with noise .  After  pa ra l l e l  
p r o c e s s i n g  by I P A  n e u r a l  n e t w o r k ,  t h e  o u t p u t  
p a t t e r n s  ( i . e . ,  t h e  p a t t e r n s  i n  t h e  2 n d  and  4 t h  
co lumns)  become comple t e  pa t t e rns  with co r rec t  
orientation. 
Due to limited size (eg., 64 neurons), the neural 
network can  only be  t r a ined  to  recognize s imple  
patterns with limited orientation. However, if we 
build a neural network with aboutlOOxlOO neurans. 
661 
-T 
Fig.4: Computer simulated results for  recognizing 
tools of different orientations: (a)  a set of training 
patterns, (b) an IWM generated with IPA model, 
(c) input/output results of the neural network. 
IV. OPTICAL IMPLEMENTATION 
O n e  o f  t h e  i m p o r t a n t  p r o p e r t i e s  o f  n e u r a l  
n e t w o r k s  i s  t h e  m a s s i v e  i n t e r c o n n e c t i o n  among  
large number of s imple processing elements  (i .e. ,  
neu rans ) .  The  2 - D  and 3 - D  pa ra l l e l  p rocess ing  
capabili t ies of optical systems make it an important 
candidate  in solving the complex interconnect ion 
problems in neural networks [8,9,10]. 
An adaptive optical neural network[8] is used to 
implement IPA model.  The optical  architecture is 
illustrated in Fig.5. In this system a high resolution 
video monitor is  used to display the weight matrix 
which was constructed with the IPA model.  This  
p roposed  sys t em d i f f e r s  f rom t h e  ma t r ix -vec to r  
processor of Farhat and Psalt is[9],  for  which the 
position of the input SLM and the IWM have been 
exchanged. We note that this arrangement makes it 
p o s s i b l e  to  use a v i d e o  mon i to r  f o r  a s s o c i a t i v e  
m e m o r y  m a t r i x  g e n e r a t i o n ,  i n s t e a d  o f  a l o w  
--7 -. . 
T h u s  t h e  sequenc ia l  e l ec t ron ic  bot t leneck can  be 
al leviated to  some extend with the feedback loop. 
Although displaying the  memory matr ices  using a 
v ideo  moni tor  i s  re la t ively s low,  however ,  t h e  
programming speed of IWM is not required to  match 
t h e  i t e r a t ion  speed  in  t h e  feedback  loop .  Wi th  
r e fe rence  to  a 1 0 2 4 x 1 0 2 4  p ixe l s  v ideo  m o n i t o r s  
(which are  available commercially), it i s  possible to  
build a parallel hybrid opt ical  neural network with 
32x32 (i.e. 1024) neurons. 
Fig.5: A 2-D hybrid optical neural network 
resolution and low contrast  SLM. A lenslet  array 
consisting of NxN small lenses is  used to establish the 
opt ical  interconnect ions between the IWM and the 
input pattern, where a moderate sized SLM with NxN 
binary p ixe l s  i s  served a s  t h e  input  device.  As 
depicted in Fig.6, the l ight beam emitted for  each 
submatrix from the video screen would be imaged by 
a specific lens  of the lenslet  array onto the input 
SLM. Thus an NxN number of submatrices would be 
added onto the input SLM. The overall transmitted 
l ight through SLM can be  imaged at  the ouput plane 
t o  form an NxN ou tpu t  array dis t r ibut ion,  which 
represents t he  product of the 4-D matrix T and the 2- 
D input pattern. The  output pattern can be picked 
up  by an NxN photo-detector array for  thresholding 
and feedback iterations. 
Video L e n s l e t  Input o u t p u t  
Monitor  A r r a y  Device De tec to r  
I I & 
Fig.6: I l lustration of optical interconnections of the 
neural network. 
To  form a closed loop neural network operation, 
the output  signals from the detector  array are  fed 
back to  the input SLM via a thresholding circuit .  It 
i s  apparent, by the intervention of a computer ,  the 
p r o p o s e d  o p t i c a l  n e u r a l  n e t w o r k  c a n  b e  m a d e  
adaptive. 
We propose that a ferroelectric liquid crystal  
SLM would be used as the input device of the system. 
Accordingly, i ts  contrast ratio can be a s  high as  
125:l [ l l ] .  S i n c e  t h e  S L M  c a n  be  addressed in  
p a r a l l e l ,  t h e  d e t e c t o r  and  t h e  inpu t  a r r a y s  c a n  
communicate  in parallel .  Thus  the electro-optical 
f eedback  loop  c a n  be  per formed in a comple t e ly  
parallel  manner. This  arrangement would allow the 
system to operate at  high-speed asynchronous mode. 
Fig.7: Experimental  demonstrat ions obtained with 
the proposed optical neural network of Fig. 5, using 
an LCTV as an input SLM: (a) three similar English 
letters as  reference patterns, (b  & c)  positive and 
negative parts of the IWMs of the IPA model, (d & e) 
positive and negative parts of the Hopfield model, 
( f )  I n p u t  p a t t e r n  , S N R  = 7 d B ,  ( g )  p a t t e r n  
r e c o n s t r u c t i o n  u s i n g  I P A  m o d e l ,  ( h )  p a t t e r n  
reconstruction obtained with Hopfield model. 
662 
Since the resolution requirement of the lenslet  
array is  rather relaxing. An array of 32x32 lenses 
with 2.5 mm in diameter can provide at least 10 times 
the resolution of a commercially the TV monitor,  
which has a resolution of about 3 lines/mm. 
However the alignment of the optical system is  
c r i t i c a l  f o r  t h e  m a t r i x - v e c t o r  o p e r a t i o n s .  T h e  
s u b m a t r i c i e s  on TV sc reen  have to  be p rec i se ly  
imaged onto the input SLM by the lenslet  array in 
superimposing position. Since the proposed optical 
neural network i s  essencially a close-loop feedback 
system, the precise alignment can be corrected by 
ad jus t ing  t h e  pos i t i on  of each  submat r ix  on TV 
screen by computer  intervent ion,  and the intensity 
of  the TV screen can also be adjusted.  Thus the 
proposed op t i ca l  system can  indeed perform in 
adaptive mode. 
In  expe r imen t s ,  Hopfield and IPA models  a re  
chosen t o  perform pattern recognition using noisy 
input patterns. ' B ' , ' P ' and ' R ' are three letters 
stored in the weight matrix as  shown in Fig. 7(a).  
The posi t ive and negat ive par ts  of IWMs f o r  IPA 
model are shown in Figs. 7(b) and (c),  while those 
for Hopfield model are displayed in Figs. 7(d) and (e). 
In comparison between these two sets of IWMs it can 
be seen that the IPA model has two major advantages 
over the Hopfield model, namely: 
1. less interconnections; 
and 2. fewer gray levels. 
T h e  l a t e r  i s  s i g n i f i c a n t  b e c a u s e  the  IPA mode l  
requires  only 3 gray levels t o  represent the IWM, 
whereas the Hopfield model needs 2M+1 gray levels, 
where M deno tes  the number of s tored r e fe rence  
patterns.  
The experimental results of these two models are 
obtained based on an input pattern ' B ' embedded in 
30% random noise (SNR = 7 dB), as shown in Fig.7(f). 
T h e  o u t p u t  p a t t e r n s  of  t h e  IPA mode l  and  t h e  
Hopf i e ld  mode l  a r e  shown  in F igs .7 (g )  and ( h )  
respectively. Because of the curvature of the video 
monitor  s c reen ,  t he  output  resul ts  are  somewhat  
dis tor ted.  Nevertheless,  the results obtained from 
the IPA model have been shown better than those 
obtained from the Hopfield model. , 
V. CONCLUSIONS 
We have i l lustrated IPA neural network model 
fo r  pattern recognition. By using a simple logical 
rule,  the common features and the special  features 
of the reference patterns can be obtained, and the 
pos i t i ve ,  nega t ive  or  no  in t e rconnec t ions  can be 
assigned to  each neuron. The IWM can be easily 
fo rmula t ed ,  which requires  merely 3 gray levels .  
An adaptive optical  neural network is used to carry 
out the parallel  processing. Computer simulations 
and experimental  results have shown that the IPA 
model  can  pe r fo rm more e f f ec t ive ly  in  t e rms  of 
pat tern recogni t ion (among s imilar  pat terns)  than 
the Hopfield model. The basic reason is  that the IPA 
mode l  p u t s  more  e m p h a s i s  on  the  i n t e r - p a t t e r n  
relationships,  while the Hopfield model deals only 
the intra-pattern association. 
ACKNOWLEDGEMENT 
We acknowledge the support of the U. S .  Army 
Research office contract DAAL03-87-0147. 
References 
[ l ] . R u m e l h a r t ,  D. E . ,  and  D. Z ipse r .  " F e a t u r e  
Discovery by competit ive Learning," Rumelhart ,  D. 
E., and McClelland. J. L., eds., Paral le l  Distributed 
Processing:  Explorat ions in  the Microstructure  of 
ition , Chapter 5 ,  Vol 1, pp151-193, MIT Press 
( 1986). 
[2]. Kohonen. T., S e l f - O r ~ ~ i o n  and Associative 
Memory, Springer-Verlag (1984). 
[3]. Fukushima,  K., "A Neural Network for  Visual 
Pattern Recognition," Computer, Vol. 21,  No. 3 ,  65 
( 1988). 
(41. Carpenter, G. A., and Grossberg, S., "A Massively 
Paral le l  Architecture fo r  a Self-organizing Neural 
Pa t t e rn  Recogn i t ion  Machine,"  Computer  V i s ion ,  
Graphics, and Image Processing, Vol. 37, 54 (1987). 
[5]. Hopfield,  J. J . ,  "Neural Network and Physical  
Sys t em wi th  Emergen t  Co l l ec t ive  Computa t iona l  
Abilities," 79, 2554 (1982). 
[6] .McClel land,  J. L.. and Rumelhart ,  D. E. ,  "An 
Interactive Activation Model of Context Effects in 
Let ter  Percept ion:  Pa r t  I. An Account of Basic  
Findings,  *I Psychological Review, Vol. 88,  No. 5 ,  
375( 1981). 
[7]. P .  S m o l e n s k y ,  " I n f o r m a t i o n  P r o c e s s i n g  i n  
Dynamical Systems: Fundations of Harmony Theory," 
Rumelhart, D. E., and McClelland, J. L., eds., Paral le l  
i s t r i b u t e d  P r o c e s s i n e :  E x D i o r a t i o n s  i n  t h e  
Microstructure of Cognitiqn . .  , Chapter 6, Vol 1, pp194- 
281, MIT Press (1986). 
[8]. Lu, T., Wu, S., Xu, X., and Yu, F. T. S., "A 2-D 
Programmable Optical Neural Network," to appear in 
Applied Optics. 
[9] .Farhat ,  N. H.,  and Psa l t i s ,  D.. "Optical  
Implementat ion of  Associat ive Memory Based on 
Mode l s  of  Neura l  Ne tworks , "  O p t i c a l  S i e n a l  
p r o c e s u ,  ed. by Horner, J. L., Academic Press, pp. 
[lo]. Athale, R. A., Szu. H. H., and Friedlander, C. B., 
"Optical Implementation of Associative Memory with 
Controlled Nonlinearity in the Correlation Domain," 
Opt. Lett., Vol. 11, No. 7, 482 (1986). 
[ l l ] .  Johnson, K. M., Handschy, M. A., and Pagano- 
S t a u f f e r ,  L.  A. ,  "Op t i ca l  Comput ing  and Image  
Processing with Ferroelectric Liquid Crystals," Opt. 
Eng.. Vol. 26, No. 5 ,  385(1987). 
. .  
. .  
. .  . .  
. .  
129-162 (1987). 
663 
