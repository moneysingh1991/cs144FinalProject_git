AN INVESTIGATION OF NEURAL NETWORKS 
I. System Description 
Richard J. McDuff? 
Patrick K. Simpson? 
David Gunning$ 
FOR F-16 FAULT DIAGNOSIS: 
We will report results of our ongoing research exploring 
the use of artificial neural networks (ANNs) for F16 flight 
line diagnostics. ANN’S are based upon the methods of in- 
formation processing that might be used in neural circuitry 
rather than those of a conventional computer. A significant 
aspect of neural networks is their ability to develop associ- 
ations simply from examples. ANN’S hold the promise of 
solving difficult logistics problems such as: multiple fault 
diagnosis, prognostication, changing configurations and en- 
vironments, and inaccurate diagnosis attributable to incom- 
plete and/or flawed rules. We tested three representative 
ANN’S to see which type worked best for our problem. 
We chose back propagation (BPN) and counterpropaga- 
tion (CPN) because they are considered to be two of the 
more promising pattern matching paradigms. The binary 
adaptive resonance theory I (ART1) was picked because 
it leams faster than CPN or BPN and has on-line adaption 
(i.e. does not have to be totally retrained every time a new 
pattern is discovered). On-line adaption is a powerful at- 
tribute, allowing new associations to be immediately incor- 
porated into the knowledge base on the flight line or wher- 
ever needed. This paper explains the advantages and 
drawbacks to each network tested and describes how we 
trained them using historical flight line data. Of the three 
ANN’S examined ART1 proved to be the most appropriate 
and was able to produce multiple symptom to multiple fault 
diagnoses. 
1. INTRODUCTION 
There are several problems that plague existing 
military diagnostic systems. Multiple fault diagnostics - 
one symptom leads to several faults, many symptoms lead 
to one fault, or many symptoms lead to many faults - is 
currently being performed with systems that are brittle and 
slow. Because of the need for time-critical flight-line re- 
sponse, available symptom data is either misinterpreted or 
unused - often leading to the incorrect removal of a sys- 
tem’s component. Every new weapon system must devel- 
op its own diagnostic system, resulting in slowly devel- 
oped and costly systems. Lastly, when a weapon system 
goes through a configuration change, a costly process of di- 
351 2; ., />0?0-035i 1. 1 .- : 
~~~~ ~~- -_ ~ ~ 
agnostic system development for the new configuration be- 
comes necessary to keep the diagnostic system up to date. 
We are examining the use of neural networks [ l ]  
as a potential method of solving these maintenance and di- 
agnostics problems. We have approached diagnostics 
from a pattern matching perspective in that we have con- 
structed an input pattern from symptoms and matched that 
symptom pattern to an appropriate output pattern that cor- 
responds to the fault that occurred. An operational sketch 
of this approach is shown in Figure 1. 
INPUT PATTERN (SYMPTOM) 
I 1 1 1 1  I 
t r t t t  OUTPUT CLASS (FAULT) 
FIGURE 1: An illustration of the neural network approach to diagnostics. Input pattem! 
(symptoms) are passed to a two-layer feedback neural network that determines whick 
class the input belongs within. Once the input class is determined the outputs (faults) as. 
saciated with that class are displayed. The most sophisticated neural network pattar 
classifiers (Adaptive Resonance Theory networks) will automatically add new classe! 
when they are needed. 
After an extensive comparative analysis of several 
key neural network paradigms, we have developed a neu- 
ral network-based diagnostics systeh that addresses 
each of the previously stated problems. This system is tar- 
geted for eventual integration with the Air Force Human 
Resources Lab’s Model-based Diagnostic Aiding System 
(MDAS). These preliminary results hold the potential 
promise for a generic weapon system diagnostic tool for 
any arbitrary weapon platform produced by any branch of 
the service (i.e. MlA1, F-16, ATF, ATA, SSN-21, etc.). 
Our efforts sought to answer eight key questions: 
(1) How should the mining data for the networks be 
prepared? 
(2) What is the most accurate way of representing the 
data? 
(3) Which neural networks are most appropriate for 
(4) What is the proper hardware to use? 
( 5 )  How well did the chosen network perform? 
(6) How can we maximize the speed of this network? 
(7) What is the optimal user interface design? 
(8) How can we provide multiple fault output probabili- 
ties that elnulate an MDAS probability table out- 
these problems? 
and 
put? 
2. DATA ACOUISITION. PREPARATION, AND 
REPRESENTATION 
2.1 DATA ACQUISITION 
The database used for training the neural networks 
is contained in the F-16 Tactical Interim CAMS And RE- 
MIS Reporting System (TICARRS) [2]. This database 
was acquired from the initial flight-line debriefing and the 
corrective actions taken. Each database entry contains in- 
formation for each discrepancy (symptom-fault pair). The 
TICARRS database was transformed into a neural net- 
work training set that consisted of binary-valued symp- 
tom-diagnosis pairs. 
The TICARRS database we received was not com- 
plete and contained many incorrectly entered discrepan- 
cies. In addition, many database entries did not contain a 
sufficient symptom description, often omitting important in- 
formation such as the state of the plane at fault time and 
any intermediate tests that were conducted that led to a 
successful diagnosis. Because of this poor set of data, it 
became imperative that the neural network was able to 
learn and classify symptoms with inherently noisy and 
missing information. 
2.2 DATA PREPARATION 
Although the TICARRS database has problems, it 
is the only source of actual F-16 symptom-diagnosis data 
(and a good representation of what can be expected from 
any modem weapon system). A large portion of the time 
spent on this project was dedicated to data preparation. 
We created a neural network training set by filtering 
through the TICARRS database and correcting or discard- 
ing unsuitable entries. 
From the TICARRS database we chose three key 
fields to represent the symptoms: 
(1) MFL - Maintenance Fault List codes; 
(2) FRC -Fault Reporting Codes; and 
(3) WD - When Discovered codes. 
A combination of these three fields is used as the input 
(symptoms) for the neural network. The MFL code is the 
only symptom acquired directly from the plane. The FRC 
code, when recorded out to the sixth digit, can represent a 
specific pilot complaint and is, therefore, useful symptom 
information. Sometimes, however, the FRC code is derived 
from the MFL code and is therefore redundant. The WD 
code is not a "formal" symptom, but is associated with the 
problem being diagnosed, therefore it is used. The remain- 
ing fault information, such as the air force base, the aircraft 
serial number, etc. was not used because of the data anal- 
ysis time constraints, although, in a fielded system it 
might be beneficial. 
Also, from the TICARRS database we chose two 
key fields to represent the fault that occurred: 
(1) WUC -Work Unit Code; and 
(2) AT - Action Taken (AT) code. 
A four digit WUC specifies which Line Replaceable Unit 
(LRU) was involved, and a five digit WUC indicates which 
Shop Replaceable Unit (SRU) was involved. The AT code 
signifies what type of action was taken on the unit repre- 
sented by the WUC. The narratives in both the discrepan- 
cy (symptom) and action taken (fault) portions of each re- 
port sometimes contain useful information but, since these 
are not formally or consistently encoded, they are not 
used. A complete description of the symptom and fault 
codes found in the TICARRS database is found in Table 1. 
We acquired six months of fire control system data 
and prepared it for neural network training. From an initial 
set of 5,182 TICARRS entries, only 1,662 usable entries 
were found to be usable. We divided the usable discrepan- 
cies (i.e. the historical data) into two groups: the training 
set - a group of all but the last three weeks of database 
entries (1464 entries), and a test set - the last three 
weeks of database entries (198 entries). Although each of 
the database entries is usable, each entry is not necessari- 
ly accurate. In many instances the repair listed in the da- 
tabase was not what actually fixed the problem. Fortu- 
352 
MFL - u a r i a s  of ha leaas followed by he numbers. There are 24 possible thm 
letter combinmions used in our system and so the vecta has 24 d o f f  bits for them. 
There PIC a maximum of 341 three Lgit numbers and so the vector has 341 d o f f  bits 
for each There are cosily a thwsand MFL codes. 
Example M F L  FCR 003 
FRC - consists of 8 digits of which we use only the middle 4. The middle four digits am- 
sist of 8 number pair and tbm two reparate digits. There are 100 possible two digit 
number wmbinmims for the pair and so the veaor has 100 possible odoff bits. There 
are 27 on/off bits e 4 1  fa the two separm leaa or m o  digits. Thae are over a thou- 
sand FRC coded p s i b l r  
Emmwlr F E  9463AFM 
WD - amsirta of 8 single digit which M cithcr be 8 lmcr or a number. There are 36 pos- 
Example WD D 
WUC - consists of a 5 or 6 digit number separated into a 2 digit number followed by 3 dig- 
iu which M eitha be 8 later or a number. The 2 digit number and the 3 single digits 
w e  each aorrd 81 is m the output vecta. 
sible leaas or digits and so there arc 36 d o f f  bits for the WD. 
ExamDlr W C  74AKB 
AT - amsina of 8 single digit which M c i b  bc 8 letter or a number and was smed as 
E m &  AT. R 
is in the output v e a a .  
TABLE 1: DESCRIPTION OF TICARRS DATABASE ENTRIES 
nately this was not a problem, our neural network ap- 
proach is able to handle such noise inherently. 
2.3 DATA REPRESENTATION 
2.3.1. Choosing a Representation Scheme 
One of the primary difficulties with choosing a neu- 
ral network pattern recognition approach to diagnostics is 
data representation. Because neural networks expect 
fixed length patterns (i.e. vectors) to be presented during 
training and use, a considerable amount of careful thought 
and planning is required to develop input (symptom) and 
output (fault) representations. 
Three representation schemes were examined for 
our data: (1) variable-unit representations, (2) intermedi- 
ate-unit representations, and (3) value-unit representa- 
tions [3]. The first scheme (variable-unit representation) 
represents each symptom, or action taken, by a single neu- 
ral network processing element (or vector component), 
whose value is the symptom code. Using this scheme a 
processing element (PE) in a neural network’s input layer 
represents all MFL numbers (lOOO+) and its value would 
represent the MFL’s ordinality. As an example, the MFL 
number 312 would be represented by a PE value of 312. 
An advantage to this representation is that it eliminates 
the need for an extensive analysis of the data to find all 
possible inputs and outputs. This representation also re- 
quires a smaller input and output vector dimensionality 
which will speed up the compute time for each pattern pre- 
sentation. Conversely, such a representation creates a 
more difficult mapping because the low dimensional pat- 
tern space is often linearly inseparable. The more nonlin- 
ear the mapping, the more difficult it is to acquire an accu- 
numbers and/or letten that are transfamed into a binuy repnrentation amenable to n w -  
ral networks. The resulting input represmution is a 555 element v h r  of binary values. 
rate mapping. It is felt that this data representation would 
require a highly nonlinear mapping, leading to extensive 
training time. Hence, this method was not pursued any 
further. 
The third representation scheme (value-unit repre- 
sentation) assigns every possible input or output a sepa- 
rate PE. This method would produce the least difficult map- 
ping because the high dimensional pattern space is the 
closest to being linearly separable. This representation, 
however, would have unacceptably slow computation 
times because the vectors are so large. In addition, such 
an approach would require enumeration of all possible in- 
puts or outputs, a task beyond the scope of this effort. Be- 
cause of the computation and data analysis requirements 
of this approach, encoding it is infeasible for our purposes. 
The second representation scheme (intermediate- 
unit representation) represents the symptoms and actions 
taken with an encoding somewhere between the first and 
third schemes. We chose this method for the input be- 
cause it represents a compromise between exhaustive da- 
ta analysis and extensive computation time. The mapping 
is assumed to be linearly inseparable, yet computationally 
achievable within the projects time constraints 
2.3.2. Input and Output Representations 
Figure 2 shows how we represented the symptom 
(input) as a 555-element binary-valued vector. Because 
the input representation is well understood and static, .the 
transformation from data base entry to binary vector was 
the only preprocessing necessary. 
The fault (output) representation was more diffi- 
cult. Because both single and multiple faults can be asso- 
ciated with the same symptom (input), the output repre- 
sentation has to maintain a running set of statistics that 
describe the likelihood of each possible fault given a partic- 
ular input. In addition, the number of faults is not known 
apriori, hence it is necessary to develop a dynamic output 
representation that can grow with the number of faults be- 
ing encoded in the network. To handle each of these issue, 
y1’ y2’ ..? Jk’ r ..I) YP -
OUTPUT PATTERN CLASSES 
we encoded the output data in a dynamically sized 3 di- 
mensional array that tabulates all faults that occur. Fig- 
ure 3 illustrates what information is stored with each pat- 
tern class processing element h the network and Figure 4 
shows the overall picture of the input and output represen- 
tation with the neural network. 
Class k Statistics: 
T a l  number of murrencen of this class: 4 
List of faults associated with this class: 
{Fault name: 
{Fault name: 
Number of occLencen: 
Number of ohmences: x,,, 
Fault “k { ~ ~ ~ ~ ~ o c c m a n ~ s :  k 
Qnk 
3. NEURAL NETWORK IMPLEMENTATION 
3.1. WHICH NEURAL NETWORK IS BEST? 
Choosing the proper neural network is essential to 
the success of this experiment. To determine which net- 
work is best suited to perform our pattern classification 
task requires a complete enumeration of the qualities de- 
sired in the overall system. These qualities include: 
ability to immediately classify symptoms; 
ability to handle noisy and incomplete data; 
ability to not favor heavily occumng symptoms 
ability to recognize novel symptoms; 
ability to immediately incorporate new symptoms; 
ability to develop new symptom classes. 
over rarely occurring symptoms; 
and 
Originally we had intended to use the backpropagation [4] 
network, but quickly realized that its inability to detect suf- 
ficiently novel symptoms and its inability to immediately 
incorporate new symptom information were too limiting for 
its use. Another neural paradigm that we considered was 
the Learning Vector Quantization network [5] and its 
counterpropagation extension [6] .  Although these net- 
works do provide an ability to determine the novelty of a 
symptom, they suffered from an inability to immediately in- 
corporate new symptoms and are not useful in this applica- 
Figure 4: The overall view of the neural network representation scheme for diagnostics. 
Ihe TICARRS data base envy is transformed into a 555-element input that represents 
;hat symptom. During both leaming and operation the symptom is classified and each 
class has a set of dynamically changing statistics as well as a list of all the faults associ- 
ated with the class. 
tion, What was needed was a network that would allow 
new patterns to be immediately incorporated (i.e. on-line 
learning) and could distinguish between novel and known 
patterns. The answer is the binary adaptive resonance 
theory network (ART1) [7]. 
3.2. BINARY ADAPTIVE RESONANCE THEORY 
(ART11 
The binary adaptive resonance theory (ART1) 
ANS - introduced by Carpenter and Grossberg [7] - is 
a two layer, nearest-neighbor classifier that stores an ar- 
bitrary number of binary spatial patterns Ak = (akl, ..., 
a kn ), k = 1, 2, ..., m, using a fast-learning version of the 
competitive learning law. ARTl learns on-line, operates 
in discrete or continuous time, and has the topology shown 
in Figure 5, where the n FX PES correspond to Ak’s com- 
ponents and the p Fy PES each represent a pattern class. 
The connections from the FX PES to each Fy PES w.. 1J form 
reference vectors. For example, the connections from the 
FX PES to the Fy PE b. J form the reference (weight) vec- 
tor W. J = (wlj, w2j, ..., w nJ .) . The connections from the Fy 
PES to the Fx PES form the pattern vectors. For example, 
the pattem vector attached to the jfh Fy PE is V.  = (v. J 11’ 
vj2, ..., Vjn>. 
Although ARTl is principally a two layer architec- 
ture, it is also transparently framed within two sub- 
systems - the attentional subsystem and the orienting 
354 
-~ 
+ + 
'kl 'kn a 
Figure 5: Topology of the binary adaptive resonance theory (ARTI) neural network. 
There are inter-layer feedback connections (w.. and vji) between the FX and F y  PES 
that facilitaw a resonation upon proper match between e n d e d  patterns V.  = (vjl, v j ~  
.__, Y - ) and input pattems Ak = (akl, a@ __., ah). The FX PES accept inputs from the 
environment and the Fy PES each represent a pattern class. During operation the F y  
PES employ an invisible on-center/off-surmund competition that is used U) choose the 
proper class for the presenred input. These lateral interactions are shown in the figure as 
shaded selfexciting (+) and neighbor-inhibiting (-) connections to emphasize this 
point. To keep the presentation uncluttered. all connections are not shown -there is ac- 
tually a connection from each FX PE to each Fy PE and vice-versa, a shaded negative 
lateral connection from each Fu PE to every other Fy PE, and a shaded positive recur- 
rent connection from every Fy PE to itself. 
subsystem. The attentional subsystem only allows the 
Fx PES to be engaged when an input pattern is present. 
The orienting subsystem removes Fy PES from the set of 
allowable winners when an insufficient memory-input 
match occurs - an operation termed short-term memory 
(STM) reset. These subsystems are implemented as in- 
herent operations during pattern processing. 
In essence, ARTl conducts a serial search through 
the encoded patterns associated with each class trying to 
find a sufficiently close match with the input pattern. If no 
class exists, a new class is made. The test for a sufficient 
match between the top-down feedback pattern and the 
bottom-up input pattern is called hypothesis testing. An 
operational overview is provided in Appendix One and the 
equations are provided in Appendix Two. 
3.3. PROBABILITY POST-PROCESSING 
By tracking the number of occurrences of each class 
in the ARTl network (see Figure 3) and by tracking the 
number of occurrences of each fault that occurred within 
each class we are able to provide a frequency-based prob- 
ability measure for each fault within a given class. In addi- 
tion, as the diagnostic system is used and the number of 
fault occurrences increases, the statistics become more re- 
liable. 
As an example, assume that we are operating the 
system and we have just classified a symptom as belong- 
ing to class 123 - y123. The hypothetical statistics asso- 
ciated with ~ 1 2 3  are shown in Figure 6 .  This class has 
only three faults associated with it: (1) FCC FAILED, (2) 
INS NEEDS SERVICE 
r 
Y 1' Y t '  -., Y1*3' '"9 Y 
OUTPUT PATTERN CLASSES 
and (3) INS FAILED. The to 
~ 
Class 123 Statlstics: 
~ o t a l  uumher of occurrences of this class: 43 
List of faults associated with this class: 
Fault {Fault: FCC FAILED 
Fault {Fault: INS NEEDS SERVICE 
Fault {Fault: INS FAILED 
Number of occurrences: 17 
Number of occurrences: 12 
Number of occurrences: 35 
Tigure 6: An example of how the output class statistics are used to provide probahilit) 
nformation concerning which fault is associated with each symptom. The statistics show 
hat this class has occurred 43 times. Of the 43 occurrences of this class, 17 times fault I 
,ccurred, 12 times fault 2 occurred and 35 times fault 3 occurred. Using this information 
he probabilities of each fault being associated with the given symptom are 39.5%. 28.6% 
md 81.4%. respectively 
number of times that this class has occurred is 43. Each 
of the three faults occurred 17, 12, and 35 times respec- 
tively. To calculate the probability of each fault simply 
requires dividing the number of occurrences for each indi- 
vidual fault by the total number of occurrences of the 
class. For example, the probability that fault 1 is associ- 
ated with the symptom is 17/43 = 0.395, or 39.5%. 
4. CONCLUSION 
We have developed an extension of ARTl that is 
used to implement an on-line learning multiple-fault di- 
agnostic system that improves its performance with 
use. ARTl was found to be the most appropriate neural 
network for this application because it is able to quickly 
incorporate new pattems, it can inherently handle noisy 
and missing data, and it provides immediate responses. 
Also, with the addition of the output pattern statistics, 
we can overcome database ambiguities - a very difficult 
problem in classical AI systems. 
ARTl is targeted for eventual use in diagnostic 
systems such as the Air Force Human Resources Lab's 
Model-based Diagnostic Aiding System (MDAS). The 
potential for this system is not strictly limited to this 
system. Because of the flexibility in construction and 
use, this system is easily applied to any weapon plat- 
form (as a generic diagnostic engine). When addressing 
the issue of flexibility, it is important to remember that 
neural networks can easily fuze data from several sen- 
sors - an ominous task for most diagnostic systems. 
Information fusion comes easily because the network 
learns to map input vectors to pattern classes irrespec- 
tive of the actual information contained within the input 
data. As an example, acoustic stress information and in- 
frared images could be combined as inputs to a heli-rotor 
diagnostics system. 
In this paper we have described the system that 
has been implemented to perform fault diagnosis of F-16 
fighters on the flight line. In part I1 of this paper [8] we 
will discuss the results of our simulation experiments with 
actual TICARRS data and we will outline some areas of fu- 
ture exploration. 
ACKNOWLEDGMENTS 
We would like to thank J. Harold McBeth and 
William Bertch for their insightful comments and helpful 
critiques of this work. This work was supported by Air 
Force Human Resources Lab - Combat Logistics Branch 
contract number F33615-87-GO15 (MIS-DD Program, 
Dave Gunning - POC). 
REFERENCES 
[ 11 Simpson, P., Artificial Neural Svstems: Foundations, 
Paradims. ADDlications and Imdementations, Pergamon 
Press: Elmsford, NY, expected late 1989 release. 
[2] TICARRS User’s Manual Organizational and Interme- 
diate Levels: Volume I and 11, Air Force Logistics Com- 
mand, LOCflLPO, Wright Patterson Air Force Base, OH, 
45433. 
[3] Walters, D., Response mapping functions: 
tion and analysis of connectionist representations, 
ceedinm of the IEEE First International Conference on 
Neural Networks: Volume 111, pp. 75-86, SOS Press, San 
Diego, CA, 1987. 
Classifica- 
[4] Werbos, P., Beyond Regression: New tools for predic- 
tion and analysis in the behavioral sciences, Harvard Uni- 
versity, Ph.D. Thesis, 1974. 
[5] Kohonen, T., Self-Organization and Associative Mem- 
m, Springer-Verlag: Berlin, 1984. 
[6] Hecht-Nielsen, R., Counterpropagation networks, & 
geedines of the IEEE First International Conference on 
Neural Networks. Volume II, pp. 19-32, 1987. 
[7] Carpenter, G. and Grossberg, S., A massively parallel 
architecture for a self-organizing neural pattern recognition 
machine, ComDuter Vision. GraDhics. and Image Under- 
standing, 37, 54-115, 1987. 
[8] McDuff, R. & Simpson, P. (1989). An investigation of 
neural networks for F-16 fault diagnosis: 11. Simulation ex- 
periments (in preparation). 
1. General Dynamics Electronics Division, P.O. Box 85310, MZ 7202-K, San Diego, CA 92138 
$ Air Force Human Resources Labs, Combat Logistics Branch, Wright-Patterson A m ,  OH 45433 
356 
- 
I) Resent an q u t  p a m  % = 
. . h) to the Fx PES. where Fx = 
(xl. , xn) Aaivity at the Fx l a y s  ... ... 
t -- . . La is denotcd by darkened PES. t t 
FY 
FX 
2) Each Fx PE acuvauon a, sends a 
signal through its long-term memory 
(LTM) connections wi to e v q  Fy 
PE bJ This flow of acuvation is 
h o l e d  by the darkened regions 
flowng from Fx to Fy t . . 
L. 
t t 
+ t 3) Each Fy PE unnpctcs with the 
others using compuitive intaactions 
until only one Fy PE remains active. 
This operation. in essene. amounu to 
finding the reference vecm W. that 
most closely matches the input 
p a w  AL. This activity is denoted 
by thc darkened Fy PES. . . t a  
4) w m g  Fy PE. say yJ. sends 
a topdoam s ~ p l  through IIS LTM 
ConnectlONI. vJi, back to FX and 
(possibly) ueates a new sa of Fx PE 
acuvauolls llus acuvity is 
repcsentcd by the darkened region 
FY 
FX 
t 
k1 w L. 
t t 
-g from Y] 
5) Themput.  Al, = . h) IS 
canpared wth the topdoam - Fy to 
. 5) arlted by the "g FB PE 
and the difference ( H m m g  
d ~ s t a n a )  is measured bctwcm Ak 
and X Thc FX activity is repnsented 
by the darkened PES and the 
comparison oprauon is denoted by 
the box 
FX 
MPUT-ToP-DOWN CoMP*RISoN 
k1 ti h 
I If the duYerence between X and Ak 
exceeds a p&m"ed  value. the FY 
FX 
vigilance parameto, then the w m m g  
Fy PE does not represent the poper 
class for Ak and it is removed from 
the sa of allowable Fy m a s .  If 
there are still Fy PES rrmmmg in 
step (2). otherwse remit an 
the sa of allowable w~nners, go to 
uncommitted Fy PE and encode Ak 
onto this PE's reference vector 
FY 
FX ca 
ENCODE PATTERN ONTO WEIGHTS 
) If the diffacncc bawem the two d v a t i a u  dou not ex& the vigilanc 
parameter, then y. is represents the pmper dass for the input pattun 4, and th 
inpu~ p.rtem is then merged with the smcd p m .  
APPENDIX Two 
BINARY A D m  RLWNAh'CE THEORY - EQUATIONS 
SYSTEM VARIABLES (Note - all vectors are binary valued): 
VI = (vJ1. vJ2. ..,, vJn) = topdown pattern (templare) recalled L ,  I 
max( I.D]<B<I+D 
A.C M 
from the winning Fy PE yr. 
Ak during steps (1)-(3) 
Ak n VJ during steps (4)-(6) 
x = (XI. x2' .... Xn) = 
INITIALIZATION: 
Dimt  A- Inequality: 
0 < w.. < L / (L-l+n) 
Template Learning Inequality: 
(B-I)D < vji S 1. 
Fy CLASS SEARCH: 
J = index set of allowable 0 otherwise 
Fy PE winnm 
Yj = 
Tk = 'i Wik 
Fy PE RESET RULE: 
Remove the winning Fy PE y, from thc sct of allowable winnm i f  
1x1 = c xi 
. ,=I 
when 0 < p S 1 is the vigilance paramter. me closer p is to 1. the finer the 
classification granularity. 
LEARNING LAWS (FAST LEARNING): 
Bottom-Up Lenrning: Only change the connections that abut the winning PE yJ: 
~ . ~ = a ~ ~  
Top-Down Laming: Only change the connections emanating from thc winning PE yJ: 
VJi = U(L-I+IXI) 
