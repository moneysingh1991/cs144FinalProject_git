A NFllBAL -0RK MODEL 
USING INTER-PAITHU? ASSOCIATION (IPA) 
Taiwei Lu, Xin Xu, Sudong Wu. and Francis T S Yu 
Department of Electrical Engineering 
The Pennsylvania State University 
University Park, PA 16802 
Outline of a Theory of 
Massively Parallel Analog Computation 
Bruce J. MacLennan 
Department of Computer Science 
University of Tennessee 
Knoxville, TN 37996-1301 
mclennan@utkcs2.cs.utk.edu 
Abstract This paper investigates a neural Field Transfornation 
network model - IPA model, in which a basic 
logical operation is used to determine the Truly Massive 
inter-pattern association (i.e., association between 
the reference patterns), and a simple rule is applied 
to construct the tri-state interconnections in the 
network. Computer simulations of the 
reconstruction of similar English letters in the 
random noise have shown a better performance by 
the IPA neural network over the Hopfield model. 
A 2-D hybrid optical neural network is used to 
show the usefulness of this new model. Since there 
are only three gray levels in the Interconnection 
Weight Matrix (IWM) , the requirement for the 
dynamic range of the Spatial Light Modulators 
(SLMs) is easily satisfied, and the interconnections 
in the proposed network are much simpler than 
those in other models. 
AI is moving into a new phase characterized by a broadened 
understanding of the nature of knowledge, and by the use of new 
computational paradigms. A sign of this transition is the growing 
interest in neurocomputers, optical computers, molecular cotnput- 
ers and other massively parallel analog computers. We have 
argued elsewhere [l, 2, 3, 41 that the new AI will augment the 
traditional deep, narrow computation with shallow, wide computa- 
tion. That is, the new AI will exploit massive parallelism Now, 
massive parallelism means different things to different people; 
massive parallelism may begin with a hundred, a thousand, or a 
million processors. On the other hand, biological evidence sug- 
gests that skillful behavior requires a very large number of proces- 
sors, so many in fact that it is infeasible to treat them individually; 
they must be treated en masse. This has motivated us to propose 
[l] the following definition of massive parallelism: A conlputational 
system is massively parallel if the number of processing elements is 
so large that it m y  conveniently be considered a contmuous quam. 
That is, a system is massively parallel if the processing elements 
can be considered a continuous mass rather than a &ete ensedle. 
How large a number is large enough to be considered a con- 
tinuous quantity? That depends on the purpose at hand. A hun- 
dred is probably never large enough; a million is probably always 
large enough; a thousand or ten thousand may be enough. One 
of the determining factors will be whether the number is large 
enough to permit the application of continuous mathematics, 
which is generally more tractable than discrete mathematics. 
We propose fhis definition of massive parallelism for a number 
of reasons. First, skillful behavior seems to require significant 
neural mass. Second, we are interested in computers, such as opt- 
ical computers and molecular computers, for which the number of 
processing elements is effectively continuous. Third, continuous 
mathematics is generally easier than discrete mathematics. And 
fourth, we want to encourage a new styIe of thinking about paral- 
lelism. Currently, we try to apply to parallel machines the thought 
habits we have acquired from thinking about sequential machines. 
This strategy works fairly well when the degree of parallelism is 
low, but it will not scale up. One cannot think individually about 
the 10" processors of a molecular computer. Rather than post- 
pone the inevitable, we think that it's time to develop a theoreti- 
cal framework for understanding massively pardel analog com- 
puters. The principal goal of this paper is to outline such a theory. 
11-596 
